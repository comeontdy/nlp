{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 检测英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def check_word(check_str):\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "print(check_word('田东亚'))\n",
    "print(check_word('math'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加载数据并且去掉词频小于3和词频高于阈值的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43848\n",
      "15508\n",
      "['世纪', '的', '古希腊', '数学家', '现在', '被', '认为', '是', '几何', '之']\n",
      "13799\n",
      "['世纪', '古希腊', '数学家', '现在', '被', '认为', '是', '几何', '之', '为']\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki_temp.txt\",'r',encoding = 'utf-8') as f0:\n",
    "    words = []\n",
    "    t = 1e-3\n",
    "    for line in f0.readlines():\n",
    "        temp_line = line.split(' ')\n",
    "        words += temp_line\n",
    "        \n",
    "    print(len(words))\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    temp_word = [word for word in words if word_counts[word] > 3 and len(word) > 0 and check_word(word)]\n",
    "    print(len(temp_word))\n",
    "    print(temp_word[:10])\n",
    "    \n",
    "    result_word = []\n",
    "    temp_counts = Counter(temp_word)\n",
    "    for word in temp_word:\n",
    "        p_w = 1 - np.sqrt(t / (temp_counts[word]/len(temp_word)))\n",
    "        #print(p_w)\n",
    "        if p_w <= 0.8:\n",
    "            result_word.append(word)\n",
    "    print(len(result_word))\n",
    "    print(result_word[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 构建映射（词和index以及index和词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13799\n",
      "[654, 926, 708, 439, 536, 548, 809, 37, 531, 362]\n"
     ]
    }
   ],
   "source": [
    "V = set(result_word)\n",
    "word_index = {word: index for index, word in enumerate(V)}\n",
    "index_word = {index: word for index, word in enumerate(V)}\n",
    "\n",
    "train_word = [word_index[w] for w in result_word]\n",
    "print(len(train_word))\n",
    "print(train_word[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 构建标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, index, window_size=2):\n",
    "    targets = []\n",
    "    forword = []\n",
    "    backword = []\n",
    "    if index - window_size > 0 and index + window_size < len(words):\n",
    "        forword = words[index-window_size : index]\n",
    "        backword = words[index+1 : index+window_size+1]\n",
    "        targets = forword + backword\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[536, 809, 37, 439]\n"
     ]
    }
   ],
   "source": [
    "lable1 = get_targets(train_word,5,2)\n",
    "print(lable1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n",
      "[439, 536, 809, 37]\n"
     ]
    }
   ],
   "source": [
    "lable1 = get_targets(train_word,5,2)\n",
    "print(train_word[5-1])\n",
    "print(lable1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_word,batch_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    train_data = []\n",
    "    index = random.sample(range(0,len(train_word)),batch_size)\n",
    "    #print(index[:10])\n",
    "    for i in index:\n",
    "        lable = get_targets(train_word,i,2)\n",
    "        x = [train_word[i]] * len(lable)\n",
    "        y = (lable)\n",
    "        train_data.append([x,y])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[177, 177, 177, 177], [144, 453, 958, 903]]\n"
     ]
    }
   ],
   "source": [
    "train_data = get_train_data(train_word,10)\n",
    "print(train_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建图\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #占位inputs和labels是训练数据\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "    tests = tf.placeholder(tf.int32,shape = [None],name = 'tests')\n",
    "    \n",
    "    #构建权重矩阵\n",
    "    vocab_size = len(train_word)\n",
    "    embedding_size = 100 # 嵌入维度\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    \n",
    "    #构建softmax\n",
    "    n_sampled = 5\n",
    "    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    # 计算negative sampling下的损失\n",
    "    #loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    cost = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 创建会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练：\n",
      "Epoch 1/10 Iteration: 300 Avg. Training loss: nan 0.0252 sec/batch\n",
      "Epoch 2/10 Iteration: 600 Avg. Training loss: 2.8024 0.0248 sec/batch\n",
      "Epoch 3/10 Iteration: 900 Avg. Training loss: 2.7043 0.0255 sec/batch\n",
      "Epoch 4/10 Iteration: 1200 Avg. Training loss: 2.6733 0.0249 sec/batch\n",
      "Epoch 5/10 Iteration: 1500 Avg. Training loss: 2.6583 0.0233 sec/batch\n",
      "Epoch 6/10 Iteration: 1800 Avg. Training loss: 2.5222 0.0277 sec/batch\n",
      "Epoch 7/10 Iteration: 2100 Avg. Training loss: 2.5819 0.0255 sec/batch\n",
      "Epoch 8/10 Iteration: 2400 Avg. Training loss: 2.6101 0.0249 sec/batch\n",
      "Epoch 9/10 Iteration: 2700 Avg. Training loss: 2.5356 0.0243 sec/batch\n",
      "Epoch 10/10 Iteration: 3000 Avg. Training loss: 2.3549 0.0258 sec/batch\n",
      "训练结束！\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    epochs = 10\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"开始训练：\")\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_train_data(train_word, 300)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            #print(x.toarray()[0])\n",
    "            #print(y.toarray()[0])\n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 300 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\"Iteration: {}\".format(iteration),\"Avg. Training loss: {:.4f}\".format(loss/300),\"{:.4f} sec/batch\".format((end-start)/300))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "        #归一化\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "    print(\"训练结束！\")\n",
    "    embed_mat = sess.run(normalized_embedding)\n",
    "    save_path = saver.save(sess, \"checkpoints/tdy.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 结果&计算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(s1,s2):\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        model_file=tf.train.latest_checkpoint('checkpoints/')\n",
    "        saver.restore(sess,model_file)\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding,word_index[s1])\n",
    "        #print(valid_embedding.eval())\n",
    "        valid_embedding1 = tf.nn.embedding_lookup(normalized_embedding,word_index[s2])\n",
    "        #print(valid_embedding1.eval())\n",
    "        vec1 = valid_embedding.eval()\n",
    "        vec2 = valid_embedding1.eval()\n",
    "        print(\"{} & {} 之间的相似度为：{}\".format(s1,s2,cosine(vec1, vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/tdy.ckpt\n",
      "数学家 & 哲学家 之间的相似度为：0.843086913228035\n"
     ]
    }
   ],
   "source": [
    "sim(\"数学家\",\"哲学家\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
